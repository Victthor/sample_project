{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"developer_guide/dev_guide/","title":"Developer guide","text":"<ul> <li> <p>Clone repository: <code>git clone https://github.com/Victthor/sample_project.git</code> </p> </li> <li> <p>Install package locally for development: <code>pip install -e .</code>     After installation, you can edit the code in <code>/src</code> folder while using regular imports to the package: <code>python   import lightning_addons</code>     see: https://pip.pypa.io/en/stable/topics/local-project-installs/ </p> </li> </ul>"},{"location":"developer_guide/documentation_guide/","title":"Documentation Guide","text":""},{"location":"developer_guide/documentation_guide/#mkdocs","title":"MkDocs","text":"<p>Packages in use for documentation purposes: </p> <p>mkdocs: documentation engine mkdocstrings: support docstrings in code mkdocstrings-python: python handler mkdocs-gen-files: automatic api generation from docstrings mkdocs-literate-nav: automatic nav config creation for code API  mkdocs-section-index: eliminate <code>__init__</code> headers in docs  mkdocs-material: custom theme</p> <p>following great recipe:  automatic-code-reference-pages catalog of plugins and mods: https://github.com/mkdocs/catalog</p>"},{"location":"developer_guide/documentation_guide/#mkdocs-commands","title":"MkDocs Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server</li> <li><code>mkdocs build</code> - Build the documentation site</li> <li><code>mkdocs gh-deploy --clean</code> - Upload documentation to GitHub Pages</li> <li><code>mkdocs -h</code> - Print help message and exit</li> </ul>"},{"location":"developer_guide/documentation_guide/#project-layout","title":"Project layout","text":"<pre><code>    mkdocs.yml    # The configuration file  \n    docs/  \n        index.md  # The documentation homepage  \n        ...       # Other markdown pages, nested folders, images and other files  \n</code></pre>"},{"location":"developer_guide/documentation_guide/#adding-new-page-not-api","title":"Adding new page (not API)","text":"<ul> <li>Add xxx.md file to the <code>docs/</code>, for nested files create appropriate folders  </li> <li>Update mkdocs.yml nav config:  </li> </ul> <p><code>yaml     nav:       - Nested name 1:         - Page name: 'nested_folder_1/xxx.md'</code></p>"},{"location":"developer_guide/documentation_guide/#api-reference-generation","title":"API reference generation","text":"<p>API generated automatically utilizing mkdocs-gen-files plugin Python script responsible for auto-generation: <code>scripts/gen_ref_pages.py</code> </p>"},{"location":"developer_guide/documentation_guide/#read-the-docs","title":"Read The Docs","text":"<p>Getting started with MkDocs</p>"},{"location":"developer_guide/steps_for_version_release/","title":"Version release guide","text":""},{"location":"developer_guide/steps_for_version_release/#next-steps-describe-standard-proces-for-version-release-utilising","title":"Next steps describe standard proces for version release utilising:","text":""},{"location":"developer_guide/steps_for_version_release/#setuptoolspiptwine","title":"setuptools+pip+twine","text":"<ul> <li>Ensure you finished all code updates/changes/bug fixes/features</li> <li>Update version number in pyproject.toml</li> <li>Update dependencies in pyproject.toml</li> <li>Update documentation</li> <li>Commit+Pull+Push all changes</li> <li>Merge into Master/Main branch and Tag version number</li> <li>Build package:<ul> <li><code>python -m build</code></li> </ul> </li> <li>Upload package to the PyPi/Pypi_test/Azure artifacts, etc...<ul> <li>Using twine: <code>python -m twine upload --repository testpypi dist/*</code></li> </ul> </li> <li>Upload documentation to GitHub pages/another:  <ul> <li><code>mkdocs gh-deploy --clean</code></li> </ul> </li> </ul> <p>Useful links: setuptools quickstart</p> <p>P.S Consider to try these package managing tools: hatch flit pdm poetry </p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>lightning_addons<ul> <li>callbacks<ul> <li>class_weights</li> </ul> </li> <li>layers<ul> <li>simple_cnn</li> </ul> </li> <li>losses<ul> <li>focal</li> </ul> </li> <li>modules<ul> <li>simple_conv</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/lightning_addons/","title":"lightning_addons","text":"<p>lightning addons includes following packages:</p> <p>callbacks</p> <p>layers</p> <p>modules</p> <p>losses</p> <p>These packages can be used with pytorch_lightning</p>"},{"location":"reference/lightning_addons/callbacks/","title":"callbacks","text":""},{"location":"reference/lightning_addons/callbacks/class_weights/","title":"class_weights","text":""},{"location":"reference/lightning_addons/callbacks/class_weights/#lightning_addons.callbacks.class_weights.ClassWeights","title":"<code>ClassWeights</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback to calculate and set class weights for imbalanced classification in a PyTorch Lightning training process.</p> <p>Parameters:</p> Name Type Description Default <code>class_weight</code> <code>Optional[Union[str, dict]]</code> <p>Specifies the method for computing class weights. It can be 'balanced' (default), a dictionary of class weights, or None. If 'balanced', it uses sklearn's <code>compute_class_weight</code> with the 'balanced' strategy. If a dictionary, it directly uses the provided class weights. If None, it defaults to equal class weights.</p> <code>'balanced'</code> <p>Methods:</p> Name Description <code>on_train_start</code> <p>pl.Trainer, pl_module: pl.LightningModule) -&gt; None: Called when the training begins. Computes class weights based on the provided or default strategy and sets the calculated weights to the <code>class_weights</code> attribute of the PyTorch Lightning module.</p> <p>Attributes:</p> Name Type Description <code>class_weight</code> <code>Optional[Union[str, dict]]</code> <p>Specifies the method for computing class weights. It can be 'balanced' (default), a dictionary of class weights, or None.</p> Note <p>The computed class weights are used to address class imbalance during training in classification tasks.</p> Source code in <code>src/lightning_addons/callbacks/class_weights.py</code> <pre><code>class ClassWeights(Callback):\n    \"\"\"\n    Callback to calculate and set class weights for imbalanced classification in a PyTorch Lightning training process.\n\n    Args:\n        class_weight: Specifies the method for computing class weights.\n            It can be 'balanced' (default), a dictionary of class weights, or None.\n            If 'balanced', it uses sklearn's `compute_class_weight` with the 'balanced' strategy.\n            If a dictionary, it directly uses the provided class weights.\n            If None, it defaults to equal class weights.\n\n    Methods:\n        on_train_start(trainer: pl.Trainer, pl_module: pl.LightningModule) -&gt; None:\n            Called when the training begins. Computes class weights based on the provided or default strategy\n            and sets the calculated weights to the `class_weights` attribute of the PyTorch Lightning module.\n\n    Attributes:\n        class_weight (Optional[Union[str, dict]]): Specifies the method for computing class weights.\n            It can be 'balanced' (default), a dictionary of class weights, or None.\n\n    Note:\n        The computed class weights are used to address class imbalance during training in classification tasks.\n    \"\"\"\n    def __init__(self, class_weight: Optional[Union[str, dict]] = 'balanced'):\n        self.class_weight = class_weight\n\n    def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -&gt; None:\n        \"\"\"\n        Method called when the training starts. Computes class weights based on the provided or default strategy\n        and sets the calculated weights to the `class_weights` attribute of the PyTorch Lightning module.\n\n        Args:\n            trainer (pl.Trainer): The PyTorch Lightning trainer.\n            pl_module (pl.LightningModule): The PyTorch Lightning module.\n\n        Returns:\n            None\n        \"\"\"\n\n        targets = trainer.datamodule._train_dataset.targets\n        class_weights = compute_class_weight(\n            class_weight=self.class_weight,\n            classes=np.unique(targets),\n            y=targets\n        )\n\n        pl_module.class_weights = torch.tensor(class_weights, dtype=pl_module.dtype, device=pl_module.device).detach()\n</code></pre>"},{"location":"reference/lightning_addons/callbacks/class_weights/#lightning_addons.callbacks.class_weights.ClassWeights.on_train_start","title":"<code>on_train_start(trainer, pl_module)</code>","text":"<p>Method called when the training starts. Computes class weights based on the provided or default strategy and sets the calculated weights to the <code>class_weights</code> attribute of the PyTorch Lightning module.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The PyTorch Lightning module.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/lightning_addons/callbacks/class_weights.py</code> <pre><code>def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -&gt; None:\n    \"\"\"\n    Method called when the training starts. Computes class weights based on the provided or default strategy\n    and sets the calculated weights to the `class_weights` attribute of the PyTorch Lightning module.\n\n    Args:\n        trainer (pl.Trainer): The PyTorch Lightning trainer.\n        pl_module (pl.LightningModule): The PyTorch Lightning module.\n\n    Returns:\n        None\n    \"\"\"\n\n    targets = trainer.datamodule._train_dataset.targets\n    class_weights = compute_class_weight(\n        class_weight=self.class_weight,\n        classes=np.unique(targets),\n        y=targets\n    )\n\n    pl_module.class_weights = torch.tensor(class_weights, dtype=pl_module.dtype, device=pl_module.device).detach()\n</code></pre>"},{"location":"reference/lightning_addons/layers/","title":"layers","text":""},{"location":"reference/lightning_addons/layers/simple_cnn/","title":"simple_cnn","text":""},{"location":"reference/lightning_addons/losses/","title":"losses","text":""},{"location":"reference/lightning_addons/losses/focal/","title":"focal","text":""},{"location":"reference/lightning_addons/losses/focal/#lightning_addons.losses.focal.MulticlassFocalLoss","title":"<code>MulticlassFocalLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>PyTorch module for computing the multiclass focal loss.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Tensor, targets: Tensor) -&gt; Tensor: Forward pass of the module. Computes the multiclass focal loss using the <code>multiclass_focal_loss</code> function.</p> <p>Attributes:</p> Name Type Description <code>weight</code> <code>Optional[Tensor]</code> <p>Weight tensor, typically representing class weights (optional).</p> <code>ignore_index</code> <code>Optional[int]</code> <p>Index to ignore in the loss calculation (optional).</p> <code>gamma</code> <code>float</code> <p>Focusing parameter for the focal term.</p> <code>reduction</code> <code>Literal['mean', 'sum', 'none']</code> <p>Specifies the reduction to apply to the loss.</p> <code>temperature</code> <code>Optional[float]</code> <p>Temperature parameter for adjusting the logits (optional).</p> Notes <p>The module wraps the <code>multiclass_focal_loss</code> function and allows using it as a part of PyTorch model architecture.</p> Source code in <code>src/lightning_addons/losses/focal.py</code> <pre><code>class MulticlassFocalLoss(torch.nn.Module):\n    \"\"\"\n    PyTorch module for computing the multiclass focal loss.\n\n    Methods:\n        forward(inputs: Tensor, targets: Tensor) -&gt; Tensor:\n            Forward pass of the module. Computes the multiclass focal loss using the `multiclass_focal_loss` function.\n\n    Attributes:\n        weight (Optional[Tensor]): Weight tensor, typically representing class weights (optional).\n        ignore_index (Optional[int]): Index to ignore in the loss calculation (optional).\n        gamma (float): Focusing parameter for the focal term.\n        reduction (Literal['mean', 'sum', 'none']): Specifies the reduction to apply to the loss.\n        temperature (Optional[float]): Temperature parameter for adjusting the logits (optional).\n\n    Notes:\n        The module wraps the `multiclass_focal_loss` function and allows using it as a part of PyTorch model architecture.\n    \"\"\"\n    def __init__(\n            self,\n            weight: Optional[Tensor] = None,  # alpha\n            ignore_index: Optional[int] = None,\n            gamma: float = 2.0,\n            reduction: Literal['mean', 'sum', 'none'] = 'mean',\n            temperature: Optional[float] = None,\n    ):\n        \"\"\"\n        Constructor for the MulticlassFocalLoss module.\n\n        Parameters:\n            weight (Optional[Tensor]): Weight tensor, typically representing class weights (optional).\n            ignore_index (Optional[int]): Index to ignore in the loss calculation (optional).\n            gamma (float): Focusing parameter for the focal term.\n            reduction (Literal['mean', 'sum', 'none']): Specifies the reduction to apply to the loss.\n                Options: 'mean', 'sum', 'none'.\n            temperature (Optional[float]): Temperature parameter for adjusting the logits (optional).\n        \"\"\"\n        super().__init__()\n\n        self.weight = weight\n        self.ignore_index = ignore_index\n        self.gamma = gamma\n        self.reduction = reduction\n        self.temperature = temperature\n\n    def forward(self, inputs: Tensor, targets: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward pass of the MulticlassFocalLoss module.\n\n        Parameters:\n            inputs (Tensor): Logits predicted by the model.\n            targets (Tensor): Ground truth class labels.\n\n        Returns:\n            Tensor: Computed focal loss.\n\n        Notes:\n            The forward pass uses the `multiclass_focal_loss` function to compute the loss.\n        \"\"\"\n        return multiclass_focal_loss(\n            inputs,\n            targets,\n            self.weight,\n            self.ignore_index,\n            self.gamma,\n            self.reduction,\n            self.temperature,\n        )\n</code></pre>"},{"location":"reference/lightning_addons/losses/focal/#lightning_addons.losses.focal.MulticlassFocalLoss.__init__","title":"<code>__init__(weight=None, ignore_index=None, gamma=2.0, reduction='mean', temperature=None)</code>","text":"<p>Constructor for the MulticlassFocalLoss module.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Optional[Tensor]</code> <p>Weight tensor, typically representing class weights (optional).</p> <code>None</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Index to ignore in the loss calculation (optional).</p> <code>None</code> <code>gamma</code> <code>float</code> <p>Focusing parameter for the focal term.</p> <code>2.0</code> <code>reduction</code> <code>Literal['mean', 'sum', 'none']</code> <p>Specifies the reduction to apply to the loss. Options: 'mean', 'sum', 'none'.</p> <code>'mean'</code> <code>temperature</code> <code>Optional[float]</code> <p>Temperature parameter for adjusting the logits (optional).</p> <code>None</code> Source code in <code>src/lightning_addons/losses/focal.py</code> <pre><code>def __init__(\n        self,\n        weight: Optional[Tensor] = None,  # alpha\n        ignore_index: Optional[int] = None,\n        gamma: float = 2.0,\n        reduction: Literal['mean', 'sum', 'none'] = 'mean',\n        temperature: Optional[float] = None,\n):\n    \"\"\"\n    Constructor for the MulticlassFocalLoss module.\n\n    Parameters:\n        weight (Optional[Tensor]): Weight tensor, typically representing class weights (optional).\n        ignore_index (Optional[int]): Index to ignore in the loss calculation (optional).\n        gamma (float): Focusing parameter for the focal term.\n        reduction (Literal['mean', 'sum', 'none']): Specifies the reduction to apply to the loss.\n            Options: 'mean', 'sum', 'none'.\n        temperature (Optional[float]): Temperature parameter for adjusting the logits (optional).\n    \"\"\"\n    super().__init__()\n\n    self.weight = weight\n    self.ignore_index = ignore_index\n    self.gamma = gamma\n    self.reduction = reduction\n    self.temperature = temperature\n</code></pre>"},{"location":"reference/lightning_addons/losses/focal/#lightning_addons.losses.focal.MulticlassFocalLoss.forward","title":"<code>forward(inputs, targets)</code>","text":"<p>Forward pass of the MulticlassFocalLoss module.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Logits predicted by the model.</p> required <code>targets</code> <code>Tensor</code> <p>Ground truth class labels.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Computed focal loss.</p> Notes <p>The forward pass uses the <code>multiclass_focal_loss</code> function to compute the loss.</p> Source code in <code>src/lightning_addons/losses/focal.py</code> <pre><code>def forward(self, inputs: Tensor, targets: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward pass of the MulticlassFocalLoss module.\n\n    Parameters:\n        inputs (Tensor): Logits predicted by the model.\n        targets (Tensor): Ground truth class labels.\n\n    Returns:\n        Tensor: Computed focal loss.\n\n    Notes:\n        The forward pass uses the `multiclass_focal_loss` function to compute the loss.\n    \"\"\"\n    return multiclass_focal_loss(\n        inputs,\n        targets,\n        self.weight,\n        self.ignore_index,\n        self.gamma,\n        self.reduction,\n        self.temperature,\n    )\n</code></pre>"},{"location":"reference/lightning_addons/losses/focal/#lightning_addons.losses.focal.multiclass_focal_loss","title":"<code>multiclass_focal_loss(inputs, targets, weight=None, ignore_index=-1, gamma=2.0, reduction='mean', temperature=None)</code>","text":"<p>Compute the multiclass focal loss.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>Logits predicted by the model.</p> required <code>targets</code> <code>Tensor</code> <p>Ground truth class labels.</p> required <code>weight</code> <code>Optional[Tensor]</code> <p>Weight tensor, typically representing class weights (optional).</p> <code>None</code> <code>ignore_index</code> <code>int</code> <p>Index to ignore in the loss calculation.</p> <code>-1</code> <code>gamma</code> <code>float</code> <p>Focusing parameter for the focal term.</p> <code>2.0</code> <code>reduction</code> <code>Literal['mean', 'sum', 'none']</code> <p>Specifies the reduction to apply to the loss. Options: 'mean', 'sum', 'none'.</p> <code>'mean'</code> <code>temperature</code> <code>Optional[float]</code> <p>Temperature parameter for adjusting the logits (optional).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Computed focal loss.</p> Notes <p>If <code>inputs</code> has more than two dimensions, it is reshaped to (N * d1 * ... * dK, C), where N is the number of samples, and C is the number of classes. The targets are also reshaped accordingly.</p> <p>If <code>ignore_index</code> is specified, positions where labels equal <code>ignore_index</code> are masked out.</p> <p>If <code>temperature</code> is provided, the logits are divided by the temperature value.</p> <p>The focal loss is calculated as a combination of negative log-likelihood (nll) loss and a focal term.</p> Source code in <code>src/lightning_addons/losses/focal.py</code> <pre><code>def multiclass_focal_loss(\n        inputs: Tensor,  # logits\n        targets: Tensor,\n        weight: Optional[Tensor] = None,  # alpha\n        ignore_index: int = -1,\n        gamma: float = 2.0,\n        reduction: Literal['mean', 'sum', 'none'] = 'mean',\n        temperature: Optional[float] = None,  # todo: separate temp of focal term and cross entropy term\n) -&gt; Tensor:\n    \"\"\"\n    Compute the multiclass focal loss.\n\n    Parameters:\n        inputs (Tensor): Logits predicted by the model.\n        targets (Tensor): Ground truth class labels.\n        weight (Optional[Tensor]): Weight tensor, typically representing class weights (optional).\n        ignore_index (int): Index to ignore in the loss calculation.\n        gamma (float): Focusing parameter for the focal term.\n        reduction (Literal['mean', 'sum', 'none']): Specifies the reduction to apply to the loss.\n            Options: 'mean', 'sum', 'none'.\n        temperature (Optional[float]): Temperature parameter for adjusting the logits (optional).\n\n    Returns:\n        Tensor: Computed focal loss.\n\n    Notes:\n        If `inputs` has more than two dimensions, it is reshaped to (N * d1 * ... * dK, C), where N is the\n        number of samples, and C is the number of classes. The targets are also reshaped accordingly.\n\n        If `ignore_index` is specified, positions where labels equal `ignore_index` are masked out.\n\n        If `temperature` is provided, the logits are divided by the temperature value.\n\n        The focal loss is calculated as a combination of negative log-likelihood (nll) loss and a focal term.\n    \"\"\"\n    if inputs.ndim &gt; 2:\n        # (N, C, d1, d2, ..., dK) --&gt; (N * d1 * ... * dK, C)\n        c = inputs.shape[1]\n        inputs = inputs.permute(0, *range(2, inputs.ndim), 1).reshape(-1, c)\n        # (N, d1, d2, ..., dK) --&gt; (N * d1 * ... * dK,)\n        targets = targets.view(-1)\n\n    # mask out the positions where labels equal the ignore_index\n    if ignore_index &gt;= 0:\n        unignored_mask = targets != ignore_index\n        targets = targets[unignored_mask]\n        if len(targets) == 0:\n            return torch.tensor(0.)\n        inputs = inputs[unignored_mask]\n\n    if temperature is not None:\n        inputs /= temperature\n\n    hot_log_probs = f.log_softmax(inputs, dim=-1)\n\n    # first calculate nll loss without reduction (including alpha == weight)\n    nll_loss = f.nll_loss(hot_log_probs, targets, weight=weight, ignore_index=ignore_index, reduction='none')\n\n    # second focal term\n    hot_log_probs = hot_log_probs[torch.arange(len(inputs)), targets]  # select probs of the target class\n    hot_probs = hot_log_probs.exp()\n    focal_term = (1 - hot_probs) ** gamma\n\n    # final product\n    loss = focal_term * nll_loss\n\n    if reduction == 'mean':\n        loss = loss.mean()\n    elif reduction == 'sum':\n        loss = loss.sum()\n\n    return loss\n</code></pre>"},{"location":"reference/lightning_addons/modules/","title":"modules","text":""},{"location":"reference/lightning_addons/modules/simple_conv/","title":"simple_conv","text":""},{"location":"reference/lightning_addons/modules/simple_conv/#lightning_addons.modules.simple_conv.ConvModel","title":"<code>ConvModel</code>","text":"<p>             Bases: <code>LightningModule</code></p> Source code in <code>src/lightning_addons/modules/simple_conv.py</code> <pre><code>class ConvModel(LightningModule):\n\n    def __init__(\n            self,\n            n_classes: int,\n            dropout: float = 0.1,\n            lr: float = 0.001,\n            class_weights: Optional[Union[\"numpy.array\", \"torch.Tensor\"]] = None\n    ):\n        \"\"\"\n\n        Args:\n            n_classes:\n            dropout:\n            lr:\n            class_weights:\n        \"\"\"\n        super().__init__()\n        self.n_classes = n_classes\n        self.lr = lr\n\n        self.model = SimpleCNN(dropout=dropout)\n\n        self.class_weights = class_weights\n        self.logits = None\n        self.targets = None\n        self.val_logits = None\n        self.val_targets = None\n\n        self.val_acc_micro = torchmetrics.classification.MulticlassAccuracy(\n            num_classes=n_classes,\n            top_k=1,\n            average='micro',\n        )\n\n        self.val_acc_macro = torchmetrics.classification.MulticlassAccuracy(\n            num_classes=n_classes,\n            top_k=1,\n            average='macro',\n        )\n\n        self.val_auc_roc = torchmetrics.classification.MulticlassAUROC(\n            num_classes=n_classes,\n            average='none',\n            thresholds=300,  # None\n        )\n\n    def forward(self, inputs):\n        return self.model(inputs)\n\n    def training_step(self, batch, batch_idx):\n\n        inputs, targets = batch\n        self.logits = self(inputs)\n        self.targets = targets\n\n        # if batch_idx == 0:\n        #     self.class_weights = torch.ones([self.n_classes, ], dtype=torch.float32, device=self.device)\n        #     self.class_weights[2] = 2.5\n        #     self.class_weights[3] = 40\n        #     self.class_weights[4] = 1.8\n        #     self.class_weights[5] = 2.0\n\n        # loss = nn.functional.cross_entropy(\n        #     self.logits.view(-1, self.logits.size(-1)),\n        #     targets.view(-1),\n        #     ignore_index=-1,\n        #     weight=self.class_weights,\n        # )\n\n        loss = multiclass_focal_loss(\n            self.logits,\n            self.targets,\n            weight=self.class_weights,\n            ignore_index=-1,\n            gamma=2.0,\n            temperature=None,\n        )\n\n        per_class_loss = torch.zeros([self.n_classes, ], dtype=self.dtype, device=self.device)\n\n        # cur_unique_classes = torch.unique(targets)\n        # cur_n_classes = cur_unique_classes.size()[0]\n        # cur_per_class_loss = torch.zeros([cur_n_classes, ], dtype=self.dtype, device=self.device)\n        #\n        # for inx, i_cur_class in enumerate(cur_unique_classes):\n        #     i_class_logits = self.logits[targets == i_cur_class]\n        #     i_targets = i_cur_class * torch.ones([i_class_logits.size()[0], ], dtype=torch.int64, device=self.device)\n        #     i_loss = nn.functional.cross_entropy(i_class_logits, i_targets, ignore_index=-1)\n        #     cur_per_class_loss[inx] = i_loss\n        #\n        # std, mean = torch.std_mean(cur_per_class_loss)\n\n        for i_class in range(self.n_classes):\n            i_class_logits = self.logits[targets == i_class]\n\n            if i_class_logits.size()[0] &gt; 0:\n                i_targets = i_class * torch.ones([i_class_logits.size()[0], ], dtype=torch.int64, device=self.device)\n                i_loss = nn.functional.cross_entropy(i_class_logits, i_targets, ignore_index=-1)\n                per_class_loss[i_class] = i_loss\n                self.log(f'train_loss_{i_class}', i_loss, on_step=True, on_epoch=True, logger=True)\n\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        inputs, targets = batch\n        logits = self(inputs)\n        self.val_logits = logits\n        self.val_targets = targets\n\n        loss = nn.functional.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            targets.view(-1),\n            ignore_index=-1,\n            # weight=self.class_weights,\n        )\n\n        per_class_loss = torch.zeros([self.n_classes, ], dtype=torch.float32, device=self.device)\n\n        for i_class in range(self.n_classes):\n            i_class_logits = logits[targets == i_class]\n\n            if i_class_logits.size()[0] &gt; 0:\n                i_targets = i_class * torch.ones([i_class_logits.size()[0], ], dtype=torch.int64, device=self.device)\n                i_loss = nn.functional.cross_entropy(i_class_logits, i_targets, ignore_index=-1)\n                per_class_loss[i_class] = i_loss\n                self.log(f'val_loss_{i_class}', i_loss, on_epoch=True, logger=True)\n\n        self.log(\"val_loss\", loss, prog_bar=True, logger=True, on_epoch=True)\n\n        aucs = self.val_auc_roc(logits, targets)\n        for i_class, auc in enumerate(aucs):\n            self.log(f'val_auc_{i_class}', auc, on_step=False, on_epoch=True)\n\n        self.val_acc_micro.update(logits, targets)\n        self.val_acc_macro.update(logits, targets)\n\n        self.log('val_acc_micro', self.val_acc_micro, on_step=False, on_epoch=True)\n        self.log('val_acc_macro', self.val_acc_macro, on_step=False, on_epoch=True)\n\n    def configure_optimizers(self):\n        # return torch.optim.SGD(self.model.parameters(), lr=self.lr)\n        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n</code></pre>"},{"location":"reference/lightning_addons/modules/simple_conv/#lightning_addons.modules.simple_conv.ConvModel.__init__","title":"<code>__init__(n_classes, dropout=0.1, lr=0.001, class_weights=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n_classes</code> <code>int</code> required <code>dropout</code> <code>float</code> <code>0.1</code> <code>lr</code> <code>float</code> <code>0.001</code> <code>class_weights</code> <code>Optional[Union[array, Tensor]]</code> <code>None</code> Source code in <code>src/lightning_addons/modules/simple_conv.py</code> <pre><code>def __init__(\n        self,\n        n_classes: int,\n        dropout: float = 0.1,\n        lr: float = 0.001,\n        class_weights: Optional[Union[\"numpy.array\", \"torch.Tensor\"]] = None\n):\n    \"\"\"\n\n    Args:\n        n_classes:\n        dropout:\n        lr:\n        class_weights:\n    \"\"\"\n    super().__init__()\n    self.n_classes = n_classes\n    self.lr = lr\n\n    self.model = SimpleCNN(dropout=dropout)\n\n    self.class_weights = class_weights\n    self.logits = None\n    self.targets = None\n    self.val_logits = None\n    self.val_targets = None\n\n    self.val_acc_micro = torchmetrics.classification.MulticlassAccuracy(\n        num_classes=n_classes,\n        top_k=1,\n        average='micro',\n    )\n\n    self.val_acc_macro = torchmetrics.classification.MulticlassAccuracy(\n        num_classes=n_classes,\n        top_k=1,\n        average='macro',\n    )\n\n    self.val_auc_roc = torchmetrics.classification.MulticlassAUROC(\n        num_classes=n_classes,\n        average='none',\n        thresholds=300,  # None\n    )\n</code></pre>"},{"location":"user_guide/how_to_use/","title":"How to use","text":""}]}